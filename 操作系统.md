## 上下文

进程上下文包括计算机系统中与执行该进程有关的各种寄存器（例如通用寄存器，程序计数器PC，程序状态字寄存器PS等）的值，

程序段在经过编译过后形成的机器指令代码集，数据集及各种堆栈值PCB结构。

### 进程上下文

包括虚拟内存、栈、全局变量等用户空间资源，还包括内核堆栈、寄存器等内核空间状态。

### 线程上下文

通常只需要切换内核堆栈、寄存器等。



## 什么引起线程切换

时间片轮转

线程阻塞

线程主动放弃时间片



## 线程和进程的区别

**进程：**每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含1--n个线程。（进程是资源分配的最小单位）

**线程：**同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小。（线程是cpu调度的最小单位）

进程，在一定的环境下，把静态的程序代码运行起来，通过使用不同的资源，来完成一定的任务。比如说，进程的环境包括环境变量，进程所掌控的资源，有中央处理器，有内存，打开的文件，映射的网络端口等等。

一个系统中，有很多进程，它们都会使用内存。为了确保内存不被别人使用，每个进程所能访问的内存都是圈好的。一人一份，谁也不干扰谁。

线程作为进程的一部分，扮演的角色就是怎么利用中央处理器去运行代码。同⼀个进程内多个线程之间可以共享代码段、数据段、打开的⽂件等资源，但每个线程各⾃都有⼀套独⽴的寄存器和栈，这样可以确保线程的控制流是相对独⽴的。

- 线程在进程下运行，一个进程可以包含多个线程
- 不同进程间数据较难共享，但不同线程间数据很易共享
- 无论是多进程的资源开销还是进程间的切换开销，进程都要比线程开销大
- 进程间不会相互影响，一个线程挂掉可能会导致整个进程挂掉





## 线程的共享资源

| 线程共享资源 | 线程独享资源 |
| :----------- | ------------ |
| 地址空间     | 程序计数器   |
| 全局变量     | 寄存器       |
| 打开的文件   | 栈           |
| 子进程       | 状态字       |



## 线程的状态

**OS的线程状态**

![img](https://images2018.cnblogs.com/blog/1418466/201808/1418466-20180813190253839-297059020.png)

![OS Thread state](https://img2020.cnblogs.com/blog/1842338/202110/1842338-20211011115632881-62622134.png)

**JVM的线程状态**

![img](https://img2020.cnblogs.com/blog/1842338/202110/1842338-20211011115638685-1098558362.jpg)





## 线程和协程

操作系统在线程等待IO的时候，会阻塞当前线程，切换到其它线程，这样在当前线程等待IO的过程中，其它线程可以继续执行。当系统线程较少的时候没有什么问题，但是当线程数量非常多的时候，却产生了问题。**一是系统线程会占用非常多的内存空间，二是过多的线程切换会占用大量的系统时间。**

协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。**协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程**，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。



 **协程相比线程的优势**

1. 协程比线程更细粒度，协程运行在线程之上，多个协程可以由一个或多个线程管理。
2. 协程上下文切换较快，少于线程之间切换的开销。协程的切换，仅仅需要改变寄存器的数值，cpu便会从需要切换的协程指定位置继续运行。
3. 同一个线程之内的协程调度不需要锁机制，因为只有一个线程，不存在同时写变量冲突，执行效率比多线程高很多。



## 进程间通信的方式

1. 管道pipe：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。
2. 命名管道FIFO：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。有名管道严格遵循**先进先出(first in first out)**。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。
3. 信号(Signal) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生；
4. 消息队列MessageQueue：消息队列是消息的[链表](),具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比 FIFO 更有优势。**消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。**
5. 共享存储SharedMemory：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。
6. 信号量Semaphore：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。
7. 套接字Socket：此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。



### 信号

信号是Linux系统中用于进程之间通信或操作的一种机制，信号可以在任何时候发送给某一进程，而无须知道该进程的状态。如果该进程并未处于执行状态，则该信号就由内核保存起来，知道该进程恢复执行并传递给他为止。如果一个信号被进程设置为阻塞，则该信号的传递被延迟，直到其阻塞被取消时才被传递给进程。

Linux提供了几十种信号，分别代表着不同的意义。信号之间依靠他们的值来区分，但是通常在程序中使用信号的名字来表示一个信号。在Linux系统中，这些信号和以他们的名称命名的常量被定义在/usr/includebitssignum.h文件中。通常程序中直接包含<signal.h>就好。

 

信号是在软件层次上对中断机制的一种模拟，是一种异步通信方式，信号可以在用户空间进程和内核之间直接交互。内核也可以利用信号来通知用户空间的进程来通知用户空间发生了哪些系统事件。信号事件有两个来源：

1）硬件来源，例如按下了cltr+C，通常产生中断信号sigint

2）软件来源，例如使用系统调用或者命令发出信号。最常用的发送信号的系统函数是kill,raise,setitimer,sigation,sigqueue函数。软件来源还包括一些非法运算等操作。

 

一旦有信号产生，用户进程对信号产生的相应有三种方式：

1）执行默认操作，linux对每种信号都规定了默认操作。

2）捕捉信号，定义信号处理函数，当信号发生时，执行相应的处理函数。

3）忽略信号，当不希望接收到的信号对进程的执行产生影响，而让进程继续执行时，可以忽略该信号，即不对信号进程作任何处理。

有两个信号是应用进程无法捕捉和忽略的，即SIGKILL和SEGSTOP，这是为了使系统管理员能在任何时候中断或结束某一特定的进程。

### 管道

管道允许在进程之间按先进先出的方式传送数据，是进程间通信的一种常见方式。

管道是Linux 支持的最初Unix IPC形式之一，具有以下特点：

1) 管道是**半双工的**，数据只能向一个方向流动；**需要双方通信时，需要建立起两个管道**；

2) 匿名管道只能用于父子进程或者兄弟进程之间（具有亲缘关系的进程）；

3) 单独构成一种独立的文件系统：管道对于管道两端的进程而言，就是一个文件，但它不是普通的文件，它不属于某种文件系统，而是自立门户，单独构成一种文件系统，并且只存在与内存中。

 

管道分为pipe（无名管道）和fifo（命名管道）两种，除了建立、打开、删除的方式不同外，这两种管道几乎是一样的。他们都是通过内核缓冲区实现数据传输。

- pipe用于相关进程之间的通信，例如父进程和子进程，它通过pipe()系统调用来创建并打开，当最后一个使用它的进程关闭对他的引用时，pipe将自动撤销。
- FIFO即命名管道，**在磁盘上有对应的节点，但没有数据块**——换言之，只是拥有一个名字和相应的访问权限，通过mknode()系统调用或者mkfifo()函数来建立的。一旦建立，任何进程都可以通过文件名将其打开和进行读写，而不局限于父子进程，当然前提是进程对FIFO有适当的访问权。当不再被进程使用时，FIFO在内存中释放，但磁盘节点仍然存在。

管道的实质是一个内核缓冲区，进程以先进先出的方式从缓冲区存取数据：管道一端的进程顺序地将进程数据写入缓冲区，另一端的进程则顺序地读取数据，该缓冲区可以看做一个循环队列，读和写的位置都是自动增加的，一个数据只能被读一次，读出以后再缓冲区都不复存在了。当缓冲区读空或者写满时，有一定的规则控制相应的读进程或写进程是否进入等待队列，当空的缓冲区有新数据写入或慢的缓冲区有数据读出时，就唤醒等待队列中的进程继续读写。

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160828225350723-1962168981.png)

### 命名管道

和无名管道的主要区别在于，命名管道有一个名字，命名管道的名字对应于一个磁盘索引节点，有了这个文件名，任何进程有相应的权限都可以对它进行访问。

而无名管道却不同，进程只能访问自己或祖先创建的管道，而不能访任意访问已经存在的管道——因为没有名字。



### 消息队列

消息队列，就是一个消息的链表，是一系列保存在内核中消息的列表。用户进程可以向消息队列添加消息，也可以向消息队列读取消息。

消息队列与管道通信相比，其优势是对每个消息指定特定的消息类型，接收的时候不需要按照队列次序，而是可以根据自定义条件接收特定类型的消息。

可以把消息看做一个记录，具有特定的格式以及特定的优先级。对消息队列有写权限的进程可以向消息队列中按照一定的规则添加新消息，对消息队列有读权限的进程可以从消息队列中读取消息。

### 共享内存

共享内存允许两个或多个进程共享一个给定的存储区，这一段存储区可以被两个或两个以上的进程映射至自身的地址空间中，一个进程写入共享内存的信息，可以被其他使用这个共享内存的进程，通过一个简单的内存读取错做读出，从而实现了进程间的通信。

采用共享内存进行通信的一个主要好处是效率高，因为进程可以直接读写内存，而不需要任何数据的拷贝，对于像管道和消息队里等通信方式，则需要再内核和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次：一次从输入文件到共享内存区，另一次从共享内存到输出文件。

一般而言，进程之间在共享内存时，并不总是读写少量数据后就解除映射，有新的通信时在重新建立共享内存区域；而是保持共享区域，直到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映射时才写回文件，因此，采用共享内存的通信方式效率非常高。

### 信号量

信号量（semaphore）与已经介绍过的 IPC 结构不同，它是一个计数器。信号量用于实现进程间的互斥与同步，而不是用于存储进程间通信数据。

1、特点
信号量用于进程间同步，若要在进程间传递数据需要结合共享内存。

信号量基于操作系统的 PV 操作，程序对信号量的操作都是原子操作。

每次对信号量的 PV 操作不仅限于对信号量值加 1 或减 1，而且可以加减任意正整数。

支持信号量组。

2、原型
最简单的信号量是只能取 0 和 1 的变量，这也是信号量最常见的一种形式，叫做二值信号量（Binary Semaphore）。而可以取多个正整数的信号量被称为通用信号量。

Linux 下的信号量函数都是在通用的信号量数组上进行操作，而不是在一个单一的二值信号量上进行操作。

### socket

IP层的ip地址可以唯一标示主机，而TCP层协议和端口号可以唯一标示主机的一个进程，这样我们可以利用ip地址＋协议＋端口号唯一标示网络中的一个进程。

能够唯一标示网络中的进程后，它们就可以利用socket进行通信了。socket是在应用层和传输层之间的一个抽象层，它把TCP/IP层复杂的操作抽象为几个简单的接口供应用层调用已实现进程在网络中通信。

------

## 锁的类型

### 互斥锁

互斥锁指代相互排斥，它是最基本的同步方式。互斥锁用于保护临界区，以保证任何时刻只有一个线程在执行其中的代码（假设互斥锁由多个线程共享），或者任何时刻只有一个进程在执行其中的代码。

**特点**

- 多个线程访问共享数据的时候是串行的
- 效率低

**Barkey锁**

- 每个线程想进入临界区之前都会升起自己的旗帜，并得到一个序号。然后升起旗帜的线程中序号最小的线程才能进入临界区。 
- 每个线程离开临界区的时候降下自己的旗帜。

### 读写锁

读写锁用于读与写之间作区分。读写锁的分配规则如下：

1. 只要没有线程持有某个给定的读写锁用于读或用于写时，那么任意数目的线程可以持有该读写锁用于读。
2. 仅当没有线程持有某个给定的读写锁用于读或用于写时，才能分配该读写锁用于写。

**特点**

- 读共享 - 并行处理，线程A加读锁成功, 又来了三个线程, 做读操作, 可以加锁成功
- 写独占，线程A加写锁成功, 又来了三个线程, 做读操作, 三个线程阻塞
- 读写不能同时，写的优先级高，线程A加读锁成功, 又来了B线程加写锁阻塞, 又来了C线程加读锁阻塞

**适用场景**

程序中的读操作>写操作的时候

### 乐观锁和悲观锁

**悲观锁**

总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。

**乐观锁**

总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量。



## 内存管理

### 物理内存

最原始的方式是直接程序直接访问物理内存，CPU 在执行指令的时候，就是通过内存地址，将物理内存上的数据载入到寄存器，然后执行机器指令。但随着发展，出现了多任务的需求，也就是希望多个任务能同时在系统上运行。这就出现了一些问题：

1. **内存访问冲突**：程序很容易出现 bug，就是 2 或更多的程序使用了同一块内存空间，导致数据读写错乱，程序崩溃。更有一些黑客利用这个缺陷来制作病毒。
2. **易产生内存碎片，内存利用率底**：有些地方的内存可能永远都用不上
3. **程序开发成本高**：程序要使用多少内存，内存地址是多少，这些都不能搞错，对于人来说，开发正确的程序很费脑子。

### 虚拟内存

我们在物理内存之上增加一个中间层，让程序通过虚拟地址去间接的访问物理内存呢。通过虚拟内存，每个进程好像都可以独占内存一样，每个进程看到的内存都是一致的，这称为虚拟地址空间。用户程序只能使用虚拟的内存地址来获取数据，系统会将这个虚拟地址翻译成实际的物理地址。

所有程序统一使用一套连续虚拟地址，比如 `0x0000 ~ 0xffff`。从程序的角度来看，它觉得自己独享了一整块内存。不用考虑访问冲突的问题。系统会将虚拟地址翻译成物理地址，从内存上加载数据。

对于内存不够用的问题，虚拟内存本质上是将磁盘当成最终存储，而主存作为了一个 cache。程序可以从虚拟内存上申请很大的空间使用，比如 `1G`；但操作系统不会真的在物理内存上开辟 `1G` 的空间，它只是开辟了很小一块，比如 `1M` 给程序使用。
 这样程序在访问内存时，操作系统看访问的地址是否能转换成物理内存地址。能则正常访问，不能则再开辟。这使得内存得到了更高效的利用。

如下图所示，每个进程所使用的虚拟地址空间都是一样的，但他们的虚拟地址会被映射到主存上的不同区域，甚至映射到磁盘上（当内存不够用时）。

![虚拟地址](https:////upload-images.jianshu.io/upload_images/11662994-c5cf238298084324.png?imageMogr2/auto-orient/strip|imageView2/2/w/664/format/webp)

其实本质上很简单，就是操作系统将程序常用的数据放到内存里加速访问，不常用的数据放在磁盘上。这一切对用户程序来说完全是透明的，用户程序可以假装所有数据都在内存里，然后通过虚拟内存地址去访问数据。在这背后，操作系统会自动将数据在主存和磁盘之间进行交换。

### 虚拟地址翻译

虚拟内存的实现方式，大多数都是通过**页表**来实现的。操作系统虚拟内存空间分成一页一页的来管理，每页的大小为 `4K`（当然这是可以配置的，不同操作系统不一样）。磁盘和主内存之间的置换也是以**页**为单位来操作的。`4K` 算是通过实践折中出来的通用值，太小了会出现频繁的置换，太大了又浪费内存。

`虚拟地址 -> 物理地址` 的映射关系由**页表（Page Table）**记录，它其实就是一个数组，数组中每个元素叫做**页表条目（Page Table Entry，简称 PTE）**，PTE 由一个有效位和 n 位地址字段构成，有效位标识这个虚拟地址是否分配了物理内存。

页表被操作系统放在物理内存的指定位置，CPU  上有个 Memory Management Unit（MMU） 单元，CPU 把虚拟地址给 MMU，MMU 去物理内存中查询页表，得到实际的物理地址。当然 MMU 不会每次都去查的，它自己也有一份缓存叫Translation Lookaside Buffer (TLB)，是为了加速地址翻译。

![虚拟地址翻译](https:////upload-images.jianshu.io/upload_images/11662994-0be170c393fd296d.png?imageMogr2/auto-orient/strip|imageView2/2/w/795/format/webp)



> 你慢慢会发现整个计算机体系里面，缓存是无处不在的，整个计算机体系就是建立在一级级的缓存之上的，无论软硬件。

让我们来看一下 CPU 内存访问的完整过程：

1. CPU 使用虚拟地址访问数据，比如执行了 MOV 指令加载数据到寄存器，把地址传递给 MMU。
2. MMU 生成 PTE 地址，并从主存（或自己的 Cache）中得到它。
3. 如果 MMU 根据 PTE 得到真实的物理地址，正常读取数据。流程到此结束。
4. 如果 PTE 信息表示没有关联的物理地址，MMU 则触发一个缺页异常。
5. 操作系统捕获到这个异常，开始执行异常处理程序。在物理内存上创建一页内存，并更新页表。
6. 缺页处理程序在物理内存中确定一个**牺牲页**，如果这个牺牲页上有数据，则把数据保存到磁盘上。
7. 缺页处理程序更新 PTE。
8. 缺页处理程序结束，再回去执行上一条指令（导致缺页异常的那个指令，也就是 MOV 指令）。这次肯定命中了。



### 内存命中率

你可能已经发现，上述的访问步骤中，从第 4 步开始都是些很繁琐的操作，频繁的执行对性能影响很大。毕竟访问磁盘是非常慢的，它会引发程序性能的急剧下降。如果内存访问到第 3 步成功结束了，我们就说**页命中**了；反之就是**未命中**，或者说**缺页**，表示它开始执行第 4 步了。

假设在 n 次内存访问中，出现命中的次数是 m，那么 `m / n * 100%` 就表示命中率，这是衡量内存管理程序好坏的一个很重要的指标。

如果物理内存不足了，数据会在主存和磁盘之间频繁交换，命中率很低，性能出现急剧下降，我们称这种现象叫**内存颠簸**。这时你会发现系统的 swap 空间利用率开始增高， CPU 利用率中 `iowait` 占比开始增高。

大多数情况下，只要物理内存够用，页命中率不会非常低，不会出现内存颠簸的情况。因为大多数程序都有一个特点，就是**局部性**。

**局部性就是说被引用过一次的存储器位置，很可能在后续再被引用多次；而且在该位置附近的其他位置，也很可能会在后续一段时间内被引用。**

前面说过计算机到处使用一级级的缓存来提升性能，归根结底就是利用了**局部性**的特征，如果没有这个特性，一级级的缓存不会有那么大的作用。所以一个局部性很好的程序运行速度会更快。



### CPU Cache

随着技术发展，CPU 的运算速度越来越快，但内存访问的速度却一直没什么突破。最终导致了 CPU 访问主存就成了整个机器的性能瓶颈。CPU Cache 的出现就是为了解决这个问题，在 CPU 和 主存之间再加了 Cache，用来缓存一块内存中的数据，而且还不只一个，现代计算机一般都有 3 级 Cache，其中 L1 Cache 的访问速度和寄存器差不多。

现在访问数据的大致的顺序是 `CPU --> L1 Cache --> L2 Cache --> L3 Cache --> 主存 --> 磁盘`。从左到右，访问速度越来越慢，空间越来越大，单位空间（比如每字节）的价格越来越低。

现在存储器的整体层次结构大致如下图：

![存储器层次结构](https:////upload-images.jianshu.io/upload_images/11662994-5c6eecbc31233544.png?imageMogr2/auto-orient/strip|imageView2/2/w/700/format/webp)

在这种架构下，缓存的命中率就更加重要了，因为系统会假定所有程序都是有局部性特征的。如果某一级出现了未命中，他就会将该级存储的数据更新成最近使用的数据。

*主存与存储器之间以 page（通常是 4K） 为单位进行交换，cache 与 主存之间是以 cache line（通常 64 byte） 为单位交换的。*



### 程序的内存布局

最后看一下程序的内存布局。现在我们知道了每个程序都有自己一套独立的地址空间可以使用，比如 `0x0000 ~ 0xffff`，但我们在用高级语言，无论是 C 还是 Go 写程序的时候，很少直接使用这些地址。我们都是通过变量名来访问数据的，编译器会自动将我们的变量名转换成真正的虚拟地址。

那最终编译出来的二进制文件，是如何被操作系统加载到内存中并执行的呢？

其实，操作系统已经将一整块内存划分好了区域，每个区域用来做不同的事情。如图：

![img](https:////upload-images.jianshu.io/upload_images/11662994-5b4e90c15b38e1b8.png?imageMogr2/auto-orient/strip|imageView2/2/w/245/format/webp)

内存布局

- **text 段：**存储程序的二进制指令，及其他的一些静态内容
- **data 段：**用来存储已被初始化的全局变量。比如常量（`const`）。
- **bss 段：**用来存放未被初始化的全局变量。和 .data 段一样都属于静态分配，在这里面的变量数据在编译就确定了大小，不释放。
- **stack 段：**栈空间，主要用于函数调用时存储临时变量的。这部分的内存是自动分配自动释放的。
- **heap 段：**堆空间，用于动态分配，C 语言中 `malloc` 和 `free` 操作的内存就在这里；Go 语言主要靠 GC 自动管理这部分。

其实现在的操作系统，进程内部的内存区域没这么简单，要比这复杂多了，比如内核区域，共享库区域。因为我们不是要真的开发一套操作系统，细节可以忽略。这里只需要记住**堆空间**和**栈空间**即可。

- **栈空间**是通过压栈出栈方式自动分配释放的，由系统管理，使用起来高效无感知。
- **堆空间**是用以动态分配的，由程序自己管理分配和释放。Go 语言虽然可以帮我们自动管理分配和释放，但是代价也是很高的。







## 虚拟内存





### 虚拟地址空间

在早期的计算机中，程序是直接运行在物理内存上的，那个时候的计算机和程序内存都很小。程序运行时会把其全部加载到内存，只要程序所需的内存不超过计算机剩余内存就不会出现问题。

 但由于程序是可以直接访问物理内存的，这也带来了内存数据的不安全性，轻则程序挂掉，重则操作系统崩溃。

 所以，我们希望程序间的内存数据是安全的互不影响的。同时计算机程序直接运行在物理内存上也导致了内存使用率较低，程序运行内存地址不确定，不同的运行顺序甚至会出错。此时在程序的执行过程中，已经存在着大量在物理内存和硬盘之间的数据交换过程。

 基于以上问题，那我们可以是不是考虑在物理内存之上增加一个中间层，让程序通过虚拟地址去间接的访问物理内存呢。通过虚拟内存，每个进程好像都可以独占内存一样，每个进程看到的内存都是一致的，这称为虚拟地址空间。（这种思想在现在也用的很广泛，例如很多优秀的中间层：Nginx、Redis等等）

 这样只要系统处理好虚拟地址到物理地址的映射关系，就可以保证不同的程序访问不同的内存区域，就可以达到物理内存地址隔离的效果，进而保证数据的安全性。



### 虚拟内存的作用

通过虚拟地址空间和页表的回顾，现在大家应该明白为什么要引入虚拟内存了吧。

虚拟内存是计算机系统内存管理的一种技术，虚拟地址空间构成虚拟内存。它使得应用程序认为它拥有连续可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片。还有部分暂时存储在外部磁盘存储器上（Swap），在需要时进行数据交换。

虚拟内存不只是用磁盘空间来扩展物理内存 的意思——这只是扩充内存级别以使其包含硬盘驱动器而已。把内存扩展到磁盘只是使用虚拟内存技术的一个结果。除了扩展内存空间，虚拟内存技术还有隔离运行内存和确定运行地址的作用。

使用虚拟内存主要是基于以下三个方面考虑，也就是说虚拟内存主要有三个作用：


- 作为内存管理工具，简化内存管理：每个进程都有统一的线性地址空间（但实际上在物理内存中可能是间隔、支离破碎的），在内存分配中没有太多限制，每个虚拟页都可以被映射到任何的物理页上。这样也带来一个好处，如果两个进程间有共享的数据，那么直接指向同一个物理页即可。


- 作为内存保护工具，隔离地址空间：进程之间不会相互影响；用户程序不能访问内核信息和代码。页表中的每个条目的高位部分是表示权限的位，MMU 可以通过检查这些位来进行权限控制（读、写、执行）。
- 作为缓存工具，提高内存利用率：使用 DRAM 当做部分的虚拟地址空间的缓存（虚拟内存就是存储在磁盘上的 N 个连续字节的数组，数组的部分内容会缓存在 DRAM 中）。扩大了内存空间，当发生缺页异常时，将会把内存和磁盘中的数据进行置换。

 

### 分段

分段机制就是把虚拟地址空间中的虚拟内存组织成一些长度可变的称为段的内存块单元。

段可以用来存放程序的代码、数据和堆栈，或者用来存放系统数据结构。

操作系统分配给进程的内存空间中包含五种段：数据段、代码段、BSS、堆、栈。

- 数据段：存放程序中的静态变量和已初始化且不为零的全局变量。


- 代码段：存放可执行文件的操作指令，代码段是只读的，不可进行写操作。这部分的区域在运行前已知其大小。


- BSS段( Block Started By Symbol)：存放未初始化的全局变量，在变量使用前由运行时初始化为零。


- 堆：存放进程运行中被动态分配的内存，其大小不固定。


- 栈：存放程序中的临时的局部变量和函数的参数值。

![640?wx_fmt=png](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9KYmlhZUtucEd1NmlhTHM5ZWs2eGZaZUxFMkd6c3RKRk5Cd1daRUx3VkNuRndRUFpOTU4wNFlwM2JuRnBMaWI0VVVKUW5PZXNVVlBQVmdveFZyUXV4UWZpYWcvNjQw?x-oss-process=image/format,png)

通过分段机制，我们可以更好的控制不同段的属性，这有利于内存的组织安排，可以对不同的属性代码、数据进行更方便的管理。如果是打乱的放在内存中，那么读写属性就很难控制。

程序运行地址和物理地址的隔离保证了程序内存数据的安全性，也解决了同一个程序运行地址不确定的问题，但是物理内存使用效率低下的问题依然没有得到解决，因为分段机制映射的是一片连续的物理内存。



### 分页

分页其实就是把段空间更细分了一下，粒度更小。此时物理内存被划分为一小块一小块，每块被称为帧(Frame)。分配内存时，帧是分配时的最小单位。

在分段方法中，每次程序的运行都会被全部加载到虚拟内存中；而分页方法则不同，单位不是整个程序，而是某个“页”，一段虚拟地址空间组成的某一页映射到一段物理地址空间组成的某一页。它将少部分要运行的代码加载到虚拟内存中，通过映射在物理内存中运行，从而提高了物理内存的使用率。

![640?wx_fmt=jpeg](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy9KYmlhZUtucEd1NmlhTHM5ZWs2eGZaZUxFMkd6c3RKRk5CZzVOM0xjU2ljYjFLbnBneThMQzNRUkJVRThlaWFyaWNGUWlhenJIZlRkWk9KSHRCeFJONkZZaktwQS82NDA?x-oss-process=image/format,png)

为了方便CPU高效执行管理物理内存，每一次都需要从虚拟内存中拿一个页的代码放到物理内存。虚拟内存页有三种状态，分别是未分配、已缓存和未缓存状态。

**未分配**：指的是未被操作系统分配或者创建的，未分配的虚拟页不存在任何数据和代码与它们关联，因此不占用磁盘资源；

**已缓存**：表示的是物理内存中已经为该部分分配的，存在虚拟内存和物理内存映射关系的；

**未缓存**：指的是已经加载到虚拟内存中的，但是未在物理内存中建立映射关系的。



**页表**

虚拟内存中的一些虚拟页是要缓存在物理内存中才能被执行的，因此操作系统存在一种机制用来判断某个虚拟页是否被缓存在物理内存中，还需要知道这个虚拟页存放在磁盘上的哪个位置，从而在物理内存中选择空闲页或者更新缓存页，并将需要的虚拟页从磁盘复制到物理内存中。这些功能是由软硬件结合完成的，其存放在物理内存中一个叫页表的数据结构中。

虚拟内存和物理内存的映射通过页表(page table)来实现。每个页表实际上是一个数组，数组中的每个元素称为页表项(PTE, page table entry)，每个页表项负责把虚拟页映射到物理页上。在物理内存中，每个进程都有自己的页表。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9KYmlhZUtucEd1NmlhTHM5ZWs2eGZaZUxFMkd6c3RKRk5CbUhqeDVleGdkMU5WdHF1UE9lTDRsaWJMc3NiN09DRTU1a0Uyek51SkpYSk9nT29uRGJWZFBpYncvNjQw?x-oss-process=image/format,png)

因为有一个表可查询，就会遇到两种情况，一种是命中(Page Hit)，另一种则是未命中(Page Fault)。

命中的时候，即访问到页表中蓝色条目的地址时，因为在 DRAM 中有对应的数据，可以直接访问。

不命中的时候，即访问到 page table 中灰色条目的时候，因为在 DRAM 中并没有对应的数据，所以需要执行缺页中断。

在上图中，四个虚拟页VP1 , VP2, VP4 , VP7 是被缓存在物理内存中。两个虚拟页VP0, VP5还未被分配。但是剩下的虚拟页VP3 ,VP6已经被分配了，但是还没有缓存到物理内存中去执行。



### 段页式

段页式存储组织是分段式和分页式结合的存储组织方法，这样可充分利用分段管理和分页管理的优点。

 (1) 用分段方法来分配和管理虚拟存储器。程序的地址空间按逻辑单位分成基本独立的段，而每一段有自己的段名，再把每段分成固定大小的若干页。

 (2) 用分页方法来分配和管理实存。即把整个主存分成与上述页大小相等的存储块，可装入作业的任何一页。程序对内存的调入或调出是按页进行的。但它又可按段实现共享和保护。



## 缺页中断

**缺页中断**

在请求分页系统中，可以通过查询页表中的状态位来确定所要访问的页面是否存在于内存中。、、每当所要访问的页面不在内存时，会产生一次缺页中断，此时操作系统会根据页表中的外存地址在外存中找到所缺的一页，将其调入内存。

缺页本身是一种中断，与一般的中断一样，需要经过4个处理步骤：

1. 保护CPU现场
2. 分析中断原因
3. 转入缺页中断处理程序进行处理
4. 恢复CPU现场，继续执行

但是缺页中断时由于所要访问的页面不存在与内存时，有硬件所产生的一种特殊的中断，因此，与一般的中断存在区别：

   1. 在指令执行期间产生和处理缺页中断信号
   2. 一条指令在执行期间，可能产生多次缺页中断
   3. 缺页中断返回时，执行产生中断的那一条指令，而一般的中断返回时，执行下一条指令

**页面置换算法**

进程运行过程中，如果发生缺页中断，而此时内存中有没有空闲的物理块是，为了能够把所缺的页面装入内存，系统必须从内存中选择一页调出到磁盘的对换区。但此时应该把那个页面换出，则需要根据一定的页面置换算法（Page Replacement Algorithm)来确定。

**常见的缓存算法**

- LRU (Least recently used) 最近最少使用，如果数据最近被访问过，那么将来被访问的几率也更高。
- LFU (Least frequently used) 最不经常使用，如果一个数据在最近一段时间内使用次数很少，那么在将来一段时间内被使用的可能性也很小。
- FIFO (Fist in first out) 先进先出， 如果一个数据最先进入缓存中，则应该最早淘汰掉。



------



## 程序的内存布局

一、**代码区**

代码区是用来储存**程序的所有代码**，以及**字符串常量等在编译期间就能确定的值**，在程序的整个生命周期内， 在常量数据区的数据都是可用的。在这个区域内，所有的数据都是只读的，不可以修改本区域的数据，之所以这样，是因为在实际的实现中，最底层内部存储格式的实现会使用特定的优化方案。比如说，编译器可能只把字符串常量存储一次，而在几个重叠的对象里面引用它 。

二、**栈区**

栈区主要存放编译器在需要的时候自动分配，在不需要的时候自动销毁的变量。主要是**局部变量和函数的参数**等，在函数调用和传参的时候，编译器为局部变量或形参开辟空间，注意，在这块空间中，编译器并不会自动对它进行任何的初始化，它所保存的不是0，而是一个随机值（可能是该储存区上次被使用后的值），在函数结束后，所开辟的空间将自动销毁，里面所存的内容将不复存在，也就是释放存储区的内容。 这就是为什么老师们在讲课中，最喜欢用的字眼：参数压栈和弹出。

三、**全局静态（数据）区**

全局静态（数据）区是用来存储**全局静态变量**的存储区域。只有在程序启动的时候才被分配，直到程序开始执行时才被初始化，比如：函数的静态变量就是在程序执行到定义该变量的代码时才被初始化的。在静态区数据区中没有被初始化的区域可以通过void* 指针来访问或操纵，但是，static定义的静态变量只能在本文件中使用，不可在其它文件中声明使用。

四、**堆区**

堆区是一个动态的存储区域，使用库函数**malloc()**和free()，和操作符**new**和delete以及一些相关变量来进行分配和回收，在堆区中，对象的生命周期可以比它村在内存中的生命周期短，换句话说：程序可以获得一片内存区域而不用马上对它进行初始化，同时，在对象被销毁后，也不用马上收回它所占用的内存区，在这段时间内，用户可以还可以用void*型的指针访问这片区域，但是原始对象的非静态区以及成员函数都不能被访问或者操纵，因为我们知道实际上对象已经不存在了。

![图四](https://img-blog.csdnimg.cn/img_convert/f108aa64f7a104628ff1f1ab07f59968.png)



### 堆和栈的区别

1、栈由系统自动分配，而堆是人为申请开辟;

2、栈获得的空间较小，而堆获得的空间较大;

3、栈由系统自动分配，速度较快，而堆一般速度比较慢;

4、栈在函数调用时，函数调用语句的下一条可执行语句的地址第一个进栈，然后函数的各个参数进栈，其中静态变量是不入栈的。而堆一般是在头部用一个字节存放堆的大小，堆中的具体内容是人为安排;

5、栈是连续的空间，而堆是不连续的空间。



堆空间不断增大可能会造成内存泄漏。分配出去的内存在使用之后没有释放掉，没有回收，长此以往，会造成没有足够的内存可以分配。一般表现为运行时间越长，占用的内存越多，最终导致系统奔溃。



## TODO IO系统

[考研复试操作系统面试题（一）-IO系统_春季觉醒的博客-CSDN博客_操作系统考研面试题](https://blog.csdn.net/qq_30719815/article/details/105075312)



## 调度

[Go 语言调度（一）: 系统调度 - 简书 (jianshu.com)](https://www.jianshu.com/p/db0aea4d60ed)



## 进程调度算法

[操作系统进程调度算法_xy的博客-CSDN博客_进程调度算法](https://blog.csdn.net/qq_36221862/article/details/55670604)

**先到先服务(FCFS)调度算法** : 从就绪队列中选择一个最先进入该队列的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 

**短作业优先(SJF)的调度算法** : 从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 

**时间片轮转调度算法** : 时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法，又称 RR(Round robin)调度。每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间。 

**多级反馈队列调度算法** ：前面介绍的几种进程调度的算法都有一定的局限性。如**短进程优先的调度算法，仅照顾了短进程而忽略了长进程** 。多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业（进程）迅速完成。，因而它是目前**被公认的一种较好的进程调度算法**，UNIX 操作系统采取的便是这种调度算法。 

**优先级调度** ： 为每个流程分配优先级，首先执行具有最高优先级的进程，依此类推。具有相同优先级的进程以 FCFS 方式执行。可以根据内存要求，时间要求或任何其他资源要求来确定优先级。

<img src="https://img-blog.csdn.net/20160605214232867" alt="img" style="zoom:150%;" />

**先来先服务**

先来先服务(FCFS)调度算法是一种最简单的调度算法，该算法既可用于作业调度，也可用于进程调度。当在作业调度中采用该算法时，每次调度都是从后备作业队列中选择一个或多个最先进入该队列的作业，将它们调入内存，为它们分配资源、创建进程，然后放入就绪队列。在进程调度中采用FCFS算法时，则每次调度是从就绪队列中选择一个最先进入该队列的进程，为之分配处理机，使之投入运行。该进程一直运行到完成或发生某事件而阻塞后才放弃处理机。

来看一个例子，假设有三个进程和它们各自执行时间（以毫秒为单位）如下表：

<img src="../img/1348147106_5559.png" alt="1348147106_5559" style="zoom:200%;" />

那么如果三个进程按照P1, P2, P3的顺序启动的话，按照先到先服务的调度算法，执行过程如下：

<img src="../img/1348147123_9218.png" alt="1348147123_9218" style="zoom:200%;" />

平均等待时间就是(0 + 24 + 27) / 3 = 17毫秒。FCFS算法是非抢占式的，一旦内核将CPU分配给一个进程就不会被释放了，除非进程结束或者请求I/O阻塞。这也是我们之前学习的多任务系统的特点。

**转轮法**

这是一种基于时钟的抢占策略，以一个周期性间隔产生时钟中断，当中断发生时，当前正在运行的进程被置于就绪队列中，然后基于FCFS策略选择下一个就绪作业的运行。这种技术也称时间片，因为每个进程在被抢占前都给定一片时间。

来看下面的例子，假设一个时间片的长度为4毫秒：

![img](https://img-my.csdn.net/uploads/201209/20/1348147300_5996.png)

**最短进程**

该算法从就绪队列中选出下一个“CPU执行期最短”的进程，为之分配处理机。
最短作业优先调度是优先级调度的特例。在优先级调度中我们根据进程的优先级来进行调度，在最短作业优先调度中我们
根据作业的执行时间长短来调度。
通过下面的例子来看看SJF是怎样调度的。

<img src="../img/1348147220_7860.png" alt="1348147220_7860" style="zoom:200%;" />

进程1首先执行了1毫秒，当执行时间更短的进程2进入Ready队列时发生抢占。进程3在进程2执行1毫秒后到来，但是进程3的
执行时间比进程2长。同理进程4的到来也没法抢占进程2，所以进程2可以一直执行到结束。之后是执行时间第二短的进程4
执行，最后是进程1（要看剩余执行时间，还剩7毫秒）和进程3。

![1348147234_1048](../img/1348147234_1048.png)

SJF调度是最优的调度，但难点在于如何预测进程的执行时间(Burst Time)。对于批处理系统中的长期调度来说，可以将用户
提交进程时输入的执行时间上限作为依据。但对于短期调度来说，没有办法能够提前得知下一个要被分配CPU的进程的执行
时间长短。我们只能通过历史数据来进行预测，公式如下：

<img src="../img/1348147255_8277.png" alt="1348147255_8277" style="zoom: 200%;" />

α可以取0.5，公式前半部分表示最近一次Burst Time，而后半部分表示过去历史平均的Burst Time。
该算法虽可获得较好的调度性能，但难以准确地知道下一个CPU执行期，而只能根据每一个进程的执行历史来预测。

**最短剩余时间**

最短剩余时间（Shortest Remaining Time,SRT）是针对SPN增加了抢占机制的版本。在这种情况下，调度程序总是选择预期剩余时间最短的进程。当一个进程加入就绪队列时，它可能比当前运行的进程具有更短的剩余时间，因此只要新进程就绪，调度程序就可能抢占当前正在运行的进程。像SPN一样，调度程序在执行选择函数时必须有关于处理时间的估计，并且存在长进程饥饿的危险。

**优先权调度算法**

1. 优先权调度算法的类型。为了照顾紧迫性作业，使之进入系统后便获得优先处理，引入了最高优先权优先（FPF）调度算法。 此算法常被用在批处理系统中，作为作业调度算法，也作为多种操作系统中的进程调度，还可以用于实时系统中。当其用于作业调度， 将后备队列中若干个优先权最高的作业装入内存。当其用于进程调度时，把处理机分配给就绪队列中优先权最高的进程，此时， 又可以进一步把该算法分成以下两种：

    1)非抢占式优先权算法

    2)抢占式优先权调度算法（高性能计算机操作系统）

  2. 优先权类型 。对于最高优先权优先调度算法，其核心在于：它是使用静态优先权还是动态优先权， 以及如何确定进程的优先权。 

  3. 高响应比优先调度算法 

为了弥补短作业优先算法的不足，我们引入动态优先权，使作业的优先等级随着等待时间的增加而以速率a提高。 该优先权变化规律可描述为：优先权=（等待时间+要求服务时间）/要求服务时间；即 =（响应时间）/要求服务时间

根据比率：R=(w+s)/s （R为响应比，w为等待处理的时间，s为预计的服务时间）

如果该进程被立即调用，则R值等于归一化周转时间（周转时间和服务时间的比率）。R最小值为1.0，只有第一个进入系统的进程才能达到该值。

调度规则为：在当前进程完成或被阻塞时，选择R值最大的就绪进程，它说明了进程的年龄。当偏向短作业时，长进程由于得不到服务，等待时间不断增加，从而增加比值，最终在竞争中胜了短进程。

和STR,SPN一样，使用最高响应比（HRRN）策略需要估计预计服务时间。

<img src="../img/1348147165_7418.png" alt="1348147165_7418" style="zoom: 150%;" />

<img src="../img/1348147192_1775.png" alt="1348147192_1775" style="zoom:150%;" />



采取基于优先级调度算法要考虑进程饿死的问题，因为高优先级的进程总是会被优先调度，具有低优先级的进程可能永远都不会被内核调度执行。Aging是对于这个问题的一个解决方案，所谓Aging就是指逐渐提高系统中长时间等待的进程的优先级。比如如果优先级的范围从127到0（127表示最低优先级），那么我们可以每15分钟将等待进程的优先级加1。最终经过一段时间，即便是拥有最低优先级127的进程也会变成系统中最高优先级的进程，从而被执行。
优先级调度可以抢占式或者非抢占式的。当一个进程在Ready队列中时，内核将它的优先级与正在CPU上执行的进程的优先级进行比较。当发现这个新进程的优先级比正在执行的进程高时：对于抢占式内核，新进程会抢占CPU，之前正在执行的进程转入Ready队列；对于非抢占式内核，新进程只会被放置在Ready队列的头部，不会抢占正在执行的进程。

**多级反馈队列调度算法**

多级反馈队列调度算法，不必事先知道各种进程所需要执行的时间，它是目前被公认的一种较好的进程调度算法。 其实施过程如下：

1) 设置多个就绪队列，并为各个队列赋予不同的优先级。在优先权越高的队列中， 为每个进程所规定的执行时间片就越小。
2) 当一个新进程进入内存后，首先放入第一队列的末尾，按FCFS原则排队等候调度。 如果他能在一个时间片中完成，便可撤离；如果未完成，就转入第二队列的末尾，在同样等待调度…… 如此下去，当一个长作业（进程）从第一队列依次将到第n队列（最后队列）后，便按第n队列时间片轮转运行。
3) 仅当第一队列空闲时，调度程序才调度第二队列中的进程运行；仅当第1到第（i-1）队列空时， 才会调度第i队列中的进程运行，并执行相应的时间片轮转。
4) 如果处理机正在处理第i队列中某进程，又有新进程进入优先权较高的队列， 则此新队列抢占正在运行的处理机，并把正在运行的进程放在第i队列的队尾。

如果没有关于各进程相对长度的任何信息，则SPN，SRT和HRRN都不能使用。另一种导致偏向短作业的方法是处罚运行时间较长的作业，换句话说，如果不能获得剩余的执行时间，那就关注已经执行了的时间。

方法为：调度基于抢占原则（按时间片）并且使用动态优先级机制。当一个进程第一次进入系统中时，它被放置在一个优先级队列中，当第一次被抢占后并返回就绪状态时，它被放置在下一个低优先级队列中，在随后的时间里，每当被抢占时，它被降级到下一个低优先级队列中。一个短进程很快会执行完，不会在就绪队列中降很多级，一个长进程会逐渐降级。因此新到的进程和短进程优先于老进程和长进程。在每个队列中，除了在优先级最低的队列中之外，都是用简单的FCFS机制，一旦一个进程处于优先级最低的队列中，它就不可能再降级，但会重复的返回该队列，知道运行结束。因此，该队列可按照轮转方式调度。

**抢占式调度算法**

1. 非抢占式调度算法

  为每一个被控对象建立一个实时任务并将它们排列成一轮转队列,调度程序每次选择队列中的第一个任务投入运行.该任务完成后便把它挂在轮转队列的队尾等待下次调度运行.

2. 非抢占式优先调度算法.

  实时任务到达时,把他们安排在就绪队列的对首,等待当前任务自我终止或运行完成后才能被调度执行.

3. 抢占式调度算法

  1）基于时钟中断的抢占式优先权调度算法.
  实时任务到达后,如果该任务的优先级别高于当前任务的优先级并不立即抢占当前任务的处理机,而是等到时钟中断到来时,调度程序才剥夺当前任务的执行,将处理机分配给新到的高优先权任务.

  2）立即抢占的优先权调度算法.
  在这种调度策略中,要求操作系统具有快速响应外部时间中断的能力.一旦出现外部中断,只要当前任务未处于临界区便立即剥夺当前任务的执行,把处理机分配给请求中断的紧迫任务，实时进程调度，实时进程抢占当前。



## 网络 IO 模型

[Java 网络IO模型简介 - 凡尘多遗梦 - 博客园 (cnblogs.com)](https://www.cnblogs.com/reecelin/p/13537734.html)



### IO 复用模型

为了解决非阻塞`IO`不断轮询导致`CPU`占用升高的问题，出现了`IO`复用模型。`IO`复用中，使用其他线程帮助去检查多个线程数据的完成情况，提高效率。

`Linux`中提供了`select`、`poll`和`epoll`三种方式来实现`IO`复用。一个线程可以对多个`IO`端口进行监听，当有读写事件产生时会分发到具体的线程进行处理。过程如下所示：

[![image-20200820172044714](https://gitee.com/workingonescape/typora_images/raw/master/image-20200820172044714.png)](https://gitee.com/workingonescape/typora_images/raw/master/image-20200820172044714.png)

`IO`复用只需要阻塞在`select`，`poll`或者`epoll`，可以同时处理和管理多个连接。缺点是当`select`、`poll`或者`epoll` 管理的连接数过少时，这种模型将退化成阻塞`IO` 模型。并且还多了一次系统调用：一次`select`、`poll`或者`epoll` 一次`recvfrom`。





## 进程切换过程

首先从用户态切换到内核态，然后要保存当前进程的状态，包括在进程表中储存寄存器值以便以后重新装载。在许多系统中，内存映像（例如，页表内的内存访问位）也必须保存。接着，通过运行调度算法选定一个新进程，之后，应该将新进程的内存映像重新装入 MMU，最后新进程开始运行。

除此之外，进程切换还要使整个内存高速缓存失效，强迫缓存从内存中动态重新装入两次（进入内核一次，离开内核一次）。



